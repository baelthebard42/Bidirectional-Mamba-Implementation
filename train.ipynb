{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "589f41f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anjal/Desktop/Code/mamba-seq-pred/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from mamba import BitShiftMamba\n",
    "from dataset import BitShiftDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from bidirectional_mamba import BiMambaBlock, BiMambaEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e6fb613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36e1bd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BitShiftDataset(bit_length=10, num_samples=10000)\n",
    "test_set = BitShiftDataset(bit_length=10, num_samples=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdad0b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset, batch_size = 32)\n",
    "test_dataloader = DataLoader(test_set, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17a2a925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiMambaEncoder(\n",
       "  (token_emb): Embedding(2, 8)\n",
       "  (layers): ModuleList(\n",
       "    (0-1): 2 x BiMambaBlock(\n",
       "      (pre_ln_f): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "      (mamba_f): Mamba(\n",
       "        (in_proj): Linear(in_features=8, out_features=32, bias=False)\n",
       "        (conv1d): Conv1d(16, 16, kernel_size=(4,), stride=(1,), padding=(3,), groups=16)\n",
       "        (act): SiLU()\n",
       "        (x_proj): Linear(in_features=16, out_features=9, bias=False)\n",
       "        (dt_proj): Linear(in_features=1, out_features=16, bias=True)\n",
       "        (out_proj): Linear(in_features=16, out_features=8, bias=False)\n",
       "      )\n",
       "      (post_ln_f): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffn_f): Sequential(\n",
       "        (0): Linear(in_features=8, out_features=32, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=32, out_features=8, bias=True)\n",
       "        (4): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (pre_ln_r): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "      (post_ln_r): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "      (mamba_r): Mamba(\n",
       "        (in_proj): Linear(in_features=8, out_features=32, bias=False)\n",
       "        (conv1d): Conv1d(16, 16, kernel_size=(4,), stride=(1,), padding=(3,), groups=16)\n",
       "        (act): SiLU()\n",
       "        (x_proj): Linear(in_features=16, out_features=9, bias=False)\n",
       "        (dt_proj): Linear(in_features=1, out_features=16, bias=True)\n",
       "        (out_proj): Linear(in_features=16, out_features=8, bias=False)\n",
       "      )\n",
       "      (ffn_r): Sequential(\n",
       "        (0): Linear(in_features=8, out_features=32, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=32, out_features=8, bias=True)\n",
       "        (4): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "  (head): Linear(in_features=8, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BiMambaEncoder(d_model=8,\n",
    "        num_layers=2,\n",
    "        d_state=4,\n",
    "        d_conv=4,\n",
    "        expand=2,\n",
    "        dropout=0.1,\n",
    "        share_ffn=False,\n",
    "        share_norm=False,).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5dc9380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 1])\n",
      "Output: tensor([1, 0, 0, 1, 1, 1, 0, 0, 1, 1])\n",
      "\n",
      "Predicted (logits): tensor([[-0.1679,  0.7027, -2.0501, -2.0176,  0.6059,  0.0115,  0.5256, -2.1343,\n",
      "         -2.1776,  0.8366]], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Predicted (sigmoid): tensor([[0.4581, 0.6688, 0.1140, 0.1174, 0.6470, 0.5029, 0.6285, 0.1058, 0.1018,\n",
      "         0.6977]], device='cuda:0', grad_fn=<SigmoidBackward0>)\n",
      "Predicted bits:0100111001\n"
     ]
    }
   ],
   "source": [
    "seq, shift = next(iter(train_dataloader))\n",
    "input = seq[0]\n",
    "output=shift[0]\n",
    "\n",
    "\n",
    "\n",
    "predicted = model(input.unsqueeze(0).to(device))\n",
    "\n",
    "print(f\"Input: {input}\\nOutput: {output}\\n\")\n",
    "print(f\"Predicted (logits): {predicted}\")\n",
    "\n",
    "print(f\"Predicted (sigmoid): {torch.sigmoid(predicted)}\")\n",
    "pred = (torch.sigmoid(predicted) > 0.5).long().squeeze(0)\n",
    "\n",
    "print(f\"Predicted bits:{''.join(map(str, pred.cpu().tolist()))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf1296c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69534924",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32f8dbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index):\n",
    "\n",
    "    running_loss = 0\n",
    "    last_loss = 0\n",
    "\n",
    "\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        seqs, shifted = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(seqs.to(device))\n",
    "\n",
    "    #    B, L, C = outputs.shape\n",
    "    #    output_logits = outputs.view(B*L, C).to(device)\n",
    "    #    target_flattened = shifted.view(B*L).to(device).long()\n",
    "\n",
    "\n",
    "    #    loss = loss_fn(output_logits, target_flattened)\n",
    "        \n",
    "        loss = loss_fn(outputs.to(device), shifted.float().to(device))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i%100 == 99:\n",
    "            last_loss = running_loss/100\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            running_loss = 0.\n",
    "    return last_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f26fb8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train( epochs=50):\n",
    "\n",
    "    best_vloss = 1_000_000.\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "     print('EPOCH {}:'.format(epoch + 1))\n",
    " \n",
    "   \n",
    "     model.train(True)\n",
    "     avg_loss = train_one_epoch(epoch)\n",
    "\n",
    "\n",
    "     running_vloss = 0.0\n",
    "    \n",
    "     model.eval()\n",
    "\n",
    "   \n",
    "     with torch.no_grad():\n",
    "        for i, vdata in enumerate(test_dataloader):\n",
    "            vinputs, vlabels = vdata\n",
    "            voutputs = model(vinputs.to(device))\n",
    "          #  B, L, C = voutputs.shape\n",
    "          #  vloss = loss_fn(voutputs.view(B*L, C).to(device), vlabels.view(B*L).to(device))\n",
    "            \n",
    "            vloss = loss_fn(voutputs.to(device), vlabels.float().to(device))\n",
    "            running_vloss += vloss\n",
    "\n",
    "     avg_vloss = running_vloss / (i + 1)\n",
    "     print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "     if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = './checkpoints/model_epoch_{}'.format(epoch)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "     \n",
    "    print(\"Training completed. Model available to use\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d589f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 100 loss: 0.1786470664292574\n",
      "  batch 200 loss: 0.07511755675077439\n",
      "  batch 300 loss: 0.07158786043524742\n",
      "LOSS train 0.07158786043524742 valid 0.07036567479372025\n",
      "EPOCH 2:\n",
      "  batch 100 loss: 0.07059268198907376\n",
      "  batch 200 loss: 0.07122941270470619\n",
      "  batch 300 loss: 0.0702647776901722\n",
      "LOSS train 0.0702647776901722 valid 0.07171255350112915\n",
      "EPOCH 3:\n",
      "  batch 100 loss: 0.07047304898500442\n",
      "  batch 200 loss: 0.07010565727949142\n",
      "  batch 300 loss: 0.06995975069701671\n",
      "LOSS train 0.06995975069701671 valid 0.06964144110679626\n",
      "EPOCH 4:\n",
      "  batch 100 loss: 0.07018087677657604\n",
      "  batch 200 loss: 0.06986536145210266\n",
      "  batch 300 loss: 0.07014162123203277\n",
      "LOSS train 0.07014162123203277 valid 0.06949552148580551\n",
      "EPOCH 5:\n",
      "  batch 100 loss: 0.0697724113613367\n",
      "  batch 200 loss: 0.06975810460746289\n",
      "  batch 300 loss: 0.06945275023579597\n",
      "LOSS train 0.06945275023579597 valid 0.06989763677120209\n",
      "EPOCH 6:\n",
      "  batch 100 loss: 0.0701641021296382\n",
      "  batch 200 loss: 0.06965983435511588\n",
      "  batch 300 loss: 0.0696184704452753\n",
      "LOSS train 0.0696184704452753 valid 0.06946031749248505\n",
      "EPOCH 7:\n",
      "  batch 100 loss: 0.06982947945594788\n",
      "  batch 200 loss: 0.0696487121284008\n",
      "  batch 300 loss: 0.06971471443772316\n",
      "LOSS train 0.06971471443772316 valid 0.06987977027893066\n",
      "EPOCH 8:\n",
      "  batch 100 loss: 0.06967368051409721\n",
      "  batch 200 loss: 0.06950845137238502\n",
      "  batch 300 loss: 0.06978662647306919\n",
      "LOSS train 0.06978662647306919 valid 0.0691651925444603\n",
      "EPOCH 9:\n",
      "  batch 100 loss: 0.0698440708220005\n",
      "  batch 200 loss: 0.06982769720256328\n",
      "  batch 300 loss: 0.06947153449058532\n",
      "LOSS train 0.06947153449058532 valid 0.06926459819078445\n",
      "EPOCH 10:\n",
      "  batch 100 loss: 0.06976950280368328\n",
      "  batch 200 loss: 0.06955509006977081\n",
      "  batch 300 loss: 0.06959889099001884\n",
      "LOSS train 0.06959889099001884 valid 0.06958435475826263\n",
      "EPOCH 11:\n",
      "  batch 100 loss: 0.06950273111462593\n",
      "  batch 200 loss: 0.06945403590798378\n",
      "  batch 300 loss: 0.06946333594620228\n",
      "LOSS train 0.06946333594620228 valid 0.06958136707544327\n",
      "EPOCH 12:\n",
      "  batch 100 loss: 0.06971309691667557\n",
      "  batch 200 loss: 0.06956949979066848\n",
      "  batch 300 loss: 0.06948141515254974\n",
      "LOSS train 0.06948141515254974 valid 0.06984390318393707\n",
      "EPOCH 13:\n",
      "  batch 100 loss: 0.06979110665619373\n",
      "  batch 200 loss: 0.06984108194708824\n",
      "  batch 300 loss: 0.0695030365139246\n",
      "LOSS train 0.0695030365139246 valid 0.06935212016105652\n",
      "EPOCH 14:\n",
      "  batch 100 loss: 0.06941324718296528\n",
      "  batch 200 loss: 0.06952574484050274\n",
      "  batch 300 loss: 0.06966482646763325\n",
      "LOSS train 0.06966482646763325 valid 0.07029112428426743\n",
      "EPOCH 15:\n",
      "  batch 100 loss: 0.06964374907314777\n",
      "  batch 200 loss: 0.06957534968852996\n",
      "  batch 300 loss: 0.06948958046734333\n",
      "LOSS train 0.06948958046734333 valid 0.06951466202735901\n",
      "EPOCH 16:\n",
      "  batch 100 loss: 0.06967318072915077\n",
      "  batch 200 loss: 0.06972096420824528\n",
      "  batch 300 loss: 0.06936604611575603\n",
      "LOSS train 0.06936604611575603 valid 0.06941589713096619\n",
      "EPOCH 17:\n",
      "  batch 100 loss: 0.06931178241968156\n",
      "  batch 200 loss: 0.069637585952878\n",
      "  batch 300 loss: 0.06950827285647393\n",
      "LOSS train 0.06950827285647393 valid 0.0693688839673996\n",
      "EPOCH 18:\n",
      "  batch 100 loss: 0.06940674714744091\n",
      "  batch 200 loss: 0.06941074885427952\n",
      "  batch 300 loss: 0.06942793622612953\n",
      "LOSS train 0.06942793622612953 valid 0.06941674649715424\n",
      "EPOCH 19:\n",
      "  batch 100 loss: 0.06962948083877564\n",
      "  batch 200 loss: 0.06952769778668881\n",
      "  batch 300 loss: 0.06953845657408238\n",
      "LOSS train 0.06953845657408238 valid 0.06933538615703583\n",
      "EPOCH 20:\n",
      "  batch 100 loss: 0.06953992694616318\n",
      "  batch 200 loss: 0.06937436111271382\n",
      "  batch 300 loss: 0.069349944293499\n",
      "LOSS train 0.069349944293499 valid 0.06929377466440201\n",
      "EPOCH 21:\n",
      "  batch 100 loss: 0.06945735909044742\n",
      "  batch 200 loss: 0.06946015268564225\n",
      "  batch 300 loss: 0.08093231335282326\n",
      "LOSS train 0.08093231335282326 valid 0.07001335918903351\n",
      "EPOCH 22:\n",
      "  batch 100 loss: 0.07747849352657794\n",
      "  batch 200 loss: 0.07050731100142002\n",
      "  batch 300 loss: 0.06995438240468502\n",
      "LOSS train 0.06995438240468502 valid 0.06952562928199768\n",
      "EPOCH 23:\n",
      "  batch 100 loss: 0.06969362445175648\n",
      "  batch 200 loss: 0.06953541107475758\n",
      "  batch 300 loss: 0.0697145190089941\n",
      "LOSS train 0.0697145190089941 valid 0.06936244666576385\n",
      "EPOCH 24:\n",
      "  batch 100 loss: 0.06958190344274044\n",
      "  batch 200 loss: 0.06950739048421382\n",
      "  batch 300 loss: 0.06976106829941273\n",
      "LOSS train 0.06976106829941273 valid 0.0692310556769371\n",
      "EPOCH 25:\n",
      "  batch 100 loss: 0.0698641648888588\n",
      "  batch 200 loss: 0.06946933209896088\n",
      "  batch 300 loss: 0.06944374077022075\n",
      "LOSS train 0.06944374077022075 valid 0.06935416907072067\n",
      "EPOCH 26:\n",
      "  batch 100 loss: 0.06949616752564908\n",
      "  batch 200 loss: 0.06949515491724015\n",
      "  batch 300 loss: 0.06971529692411423\n",
      "LOSS train 0.06971529692411423 valid 0.0694468691945076\n",
      "EPOCH 27:\n",
      "  batch 100 loss: 0.06940905548632145\n",
      "  batch 200 loss: 0.06969777017831802\n",
      "  batch 300 loss: 0.06962288416922092\n",
      "LOSS train 0.06962288416922092 valid 0.06935319304466248\n",
      "EPOCH 28:\n",
      "  batch 100 loss: 0.06961349681019784\n",
      "  batch 200 loss: 0.0696405889838934\n",
      "  batch 300 loss: 0.06961084119975566\n",
      "LOSS train 0.06961084119975566 valid 0.06963157653808594\n",
      "EPOCH 29:\n",
      "  batch 100 loss: 0.06942992746829986\n",
      "  batch 200 loss: 0.0695259253680706\n",
      "  batch 300 loss: 0.06949995502829552\n",
      "LOSS train 0.06949995502829552 valid 0.06996992975473404\n",
      "EPOCH 30:\n",
      "  batch 100 loss: 0.06952974013984203\n",
      "  batch 200 loss: 0.0694946165382862\n",
      "  batch 300 loss: 0.06939646415412426\n",
      "LOSS train 0.06939646415412426 valid 0.0691719502210617\n",
      "Training completed. Model available to use\n"
     ]
    }
   ],
   "source": [
    "train(epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "781d873e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 1])\n",
      "Output: tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 0])\n",
      "\n",
      "Predicted (logits): tensor([[-12.3343,  12.1840, -12.3455, -12.3638, -12.3470, -12.3511, -12.3607,\n",
      "         -12.3202,  12.1719,  -0.0603]], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Predicted (sigmoid): tensor([[4.3982e-06, 9.9999e-01, 4.3493e-06, 4.2704e-06, 4.3428e-06, 4.3248e-06,\n",
      "         4.2838e-06, 4.4606e-06, 9.9999e-01, 4.8492e-01]], device='cuda:0',\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "Predicted bits:0100000010\n"
     ]
    }
   ],
   "source": [
    "seq, shift = next(iter(train_dataloader))\n",
    "input = seq[0]\n",
    "output=shift[0]\n",
    "\n",
    "\n",
    "\n",
    "predicted = model(input.unsqueeze(0).to(device))\n",
    "\n",
    "print(f\"Input: {input}\\nOutput: {output}\\n\")\n",
    "print(f\"Predicted (logits): {predicted}\")\n",
    "\n",
    "print(f\"Predicted (sigmoid): {torch.sigmoid(predicted)}\")\n",
    "\n",
    "pred = (torch.sigmoid(predicted) > 0.5).long().squeeze(0)\n",
    "\n",
    "print(f\"Predicted bits:{''.join(map(str, pred.cpu().tolist()))}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
